---
title: '[TensorFlow 2.0] Optimizer 및 Training (Keras)'
categories:
  - Programming
  - AI
tags:
  - AI
  - DeepLearning
  - Tensorflow
date: 2020-07-24 17:35:22
thumbnail: /images/thumbnail/tensorflow.png
---

## Load Packages

```python
import tensorflow as tf
from tensorflow.keras import layers

from tensorflow.keras import datasets
```

## 학습 과정 돌아보기

![](/images/ai/dev/18.png)

## Prepare MNIST Datset

```python
(train_x, train_y), (test_x, test_y) = datasets.mnist.load_data()
```

## Build Model

![](/images/ai/dev/17.png)

```python
inputs = layers.Input((28, 28, 1))
net = layers.Conv2D(32, (3, 3), padding='SAME')(inputs)
net = layers.Activation('relu')(net)
net = layers.Conv2D(32, (3, 3), padding='SAME')(net)
net = layers.Activation('relu')(net)
net = layers.MaxPooling2D(pool_size=(2, 2))(net)
net = layers.Dropout(0.25)(net)

net = layers.Conv2D(64, (3, 3), padding='SAME')(net)
net = layers.Activation('relu')(net)
net = layers.Conv2D(64, (3, 3), padding='SAME')(net)
net = layers.Activation('relu')(net)
net = layers.MaxPooling2D(pool_size=(2, 2))(net)
net = layers.Dropout(0.25)(net)

net = layers.Flatten()(net)
net = layers.Dense(512)(net)
net = layers.Activation('relu')(net)
net = layers.Dropout(0.5)(net)
net = layers.Dense(10)(net)  # num_classes
net = layers.Activation('softmax')(net)

model = tf.keras.Model(inputs=inputs, outputs=net, name='Basic_CNN')
```

## Optimization

모델을 학습하기 전에 설정을 합니다.

- Loss Function
- Metrics
- Optimizer

### Loss Function

#### Categorical vs Binary

```python
loss = 'binary_crossentropy'
loss = 'categorical_crossentropy'
```

#### sparse_categorical_crossentropy vs categorical_crossentropy

```python
loss_fun = tf.keras.losses.sparse_categorical_crossentropy

# 또는
tf.keras.losses.categorical_crossentropy
tf.keras.losses.binary_crossentropy
```

### Metrics

모델을 평가하는 방법입니다.(정확도 측정)

- Accuracy : 예측이 레이블과 일치하는 빈도를 계산
- BinaryAccuracy: 예측이 이진 레이블과 일치하는 빈도를 계산
- CategoricalAccuracy : 예측이 one-hot 레이블과 일치하는 빈도를 계산
- TopKCategoricalAccuracy : 상위 K 예측에서 대상이 얼마나 자주 나타되는지 계산
- SparseTopKCategoricalAccuracy : 상위 K 예측에서 정수 대상이 얼마나 자주 나타하는지 계산

```python
# accurany를 이름으로 넣는 방법
metrics = ['accuracy']

# tf.keras.metrics
metrics = [tf.keras.metrics.CategoricalAccuracy()]
```

### Optimizer

- 'sgd'
- 'rmsprop'
- 'adam'

```python
optm = tf.keras.optimizers.Adam()

# Out
tf.keras.optimizers.SGD()
tf.keras.optimizers.RMSprop()
tf.keras.optimizers.Adam()
```

### Compile

Optimizer 적용을 합니다.

```python
model.compile(optimizer=optm,
              loss=loss_fun,
              metrics=metrics)
```

## Prepaer Dataset

학습에 사용할 데이터셋을 준비합니다.

데이터셋 확인

```python
train_x.shape, train_y.shape
> ((60000, 28, 28), (60000,))

test_x.shape, test_y.shape
> ((10000, 28, 28), (10000,))
```

차원 수 늘리기

```python
import numpy as np

np.expand_dims(train_x, -1).shape
> (60000, 28, 28, 1)

tf.expand_dims(train_x, -1).shape
> TensorShape([60000, 28, 28, 1])
```

```python
# 최신 버전
train_x = train_x[..., tf.newaxis]
test_x = test_x[..., tf.newaxis]
```

Rescaling

```python
np.min(train_x), np.max(train_x)
> (0, 255)

train_x = train_x / 255.
test_x = test_x / 255.

np.min(train_x), np.max(train_x)
> (0.0, 1.0)
```

## Training

본격적으로 학습을 해봅니다. 그 전에 학습용 Hyperparameter 설정을 합니다.

num_epochs, batch_size

```python
# 전체 sample 데이터를 이용하여 한 바퀴 돌며 학습하는 것을 1회 epoch이라 부릅니다.
num_epochs = 1

# 모델에 한번에 들어가는 데이터 수를 조절하여 메모리 사용 효율을 향상시킵니다.
batch_size = 32
```

실행!

```python
model.fit(train_x, train_y,
          batch_size=batch_size,
          shuffle=True,
          epochs=num_epochs)

# Result
Train on 60000 samples
60000/60000 [==============================] - 99s 2ms/sample - loss: 0.1414 - categorical_accuracy: 0.0992
```
